{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: RNC and DNC Speech Analysis\n",
    "* The RNC and DNC speeches are written with the intent to persuade an otherwise already persuaded audience.\n",
    "    * For example, viewers of the RNC are typically republican: \"More viewers tuned into Fox News to watch the final night of the Republican convention than any other TV network\" (https://www.wsj.com/articles/rnc-ratings-final-night-draws-23-8-million-viewers-as-trump-accepts-nomination-11598652831)\n",
    "* This analysis will include a classifier which will predict whether a test document is from a republican speaker or a democratic speaker. \n",
    "    * Goal: to see if the classifier will be able to recognize differences in the word choices of each group. Hypothesis: high accuracy. Also, see the top features.\n",
    "* Additionally, I will include a model to see if the length of the speech, the length of the sentences, the TTR, etc. has any significance on predicting between republican and democratic speeches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [First Section](#First-Section): importing all needed files and packs\n",
    "* [Second Section](#Second-Section): using svc classifier to predict R or D with text data\n",
    "* [Third Section](#Third-Section): testing out logisitic regression with presidential debate\n",
    "* [Fourth Section](#Fourth-Section): using gridsearch and cv to see the best accuracy with best parameters when certain words are cut out\n",
    "* [Fifth Section](#Fifth-Section): finding most informative features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns        # seaborn graphical package\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    30\n",
       "D    21\n",
       "Name: Aff, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_speeches = pd.read_csv(\"/Users/emmatarcson/Documents/data_science/RhetoricalFactor-analysis/data_sample/convspeeches.csv\")\n",
    "conv_speeches = conv_speeches[['Aff','Speech']]\n",
    "conv_speeches.Aff.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv_speeches['Speech'], conv_speeches['Aff'], random_state=0,\n",
    "                                  train_size=0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, max_features=2000, stop_words='english')\n",
    "\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100000.0, kernel='linear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svcmodel = SVC(kernel='linear', C=1E5)  \n",
    "svcmodel.fit(X_text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = svcmodel.predict(X_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmatarcson/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['D', 'R'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = ['D','R']\n",
    "mat = confusion_matrix(y_test, pred, labels)\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cmap=\"Reds\",\n",
    "           xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So there is one instance of it guessing Democratic, instead of Republican in the test set. \n",
    "* this makes me wonder if there is some super significant word in these speeches that is causing this, but vvv\n",
    "* That being said, I did expect a higher accuracy from the speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Toks</th>\n",
       "      <th>Token_count</th>\n",
       "      <th>Type_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "      <td>[good, evening, and, thank, you, to, everyone,...</td>\n",
       "      <td>193</td>\n",
       "      <td>118</td>\n",
       "      <td>0.611399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "      <td>[we, climbed, the, impossible, mountain, and, ...</td>\n",
       "      <td>750</td>\n",
       "      <td>326</td>\n",
       "      <td>0.434667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "      <td>[hello, america, im, andrew, yang, you, might,...</td>\n",
       "      <td>428</td>\n",
       "      <td>214</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "      <td>[good, evening, everybody, as, youve, seen, by...</td>\n",
       "      <td>2278</td>\n",
       "      <td>795</td>\n",
       "      <td>0.348990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "      <td>[good, evening, our, great, nation, is, now, l...</td>\n",
       "      <td>982</td>\n",
       "      <td>440</td>\n",
       "      <td>0.448065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Aff                                             Speech  \\\n",
       "0   D  Good evening and thank you to everyone here to...   \n",
       "1   D  We climbed the impossible mountain, and right ...   \n",
       "2   D  Hello, America. I'm Andrew Yang. You might kno...   \n",
       "3   D  Good evening everybody. As you've seen by now ...   \n",
       "4   D  Good evening. Our great nation is now living i...   \n",
       "\n",
       "                                                Toks  Token_count  Type_count  \\\n",
       "0  [good, evening, and, thank, you, to, everyone,...          193         118   \n",
       "1  [we, climbed, the, impossible, mountain, and, ...          750         326   \n",
       "2  [hello, america, im, andrew, yang, you, might,...          428         214   \n",
       "3  [good, evening, everybody, as, youve, seen, by...         2278         795   \n",
       "4  [good, evening, our, great, nation, is, now, l...          982         440   \n",
       "\n",
       "        TTR  \n",
       "0  0.611399  \n",
       "1  0.434667  \n",
       "2  0.500000  \n",
       "3  0.348990  \n",
       "4  0.448065  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "trans_toks = [nltk.word_tokenize(re.sub(r'[^\\w+ ]', '', t.lower())) for t in conv_speeches['Speech']]\n",
    "trans_tok_lens = [len(t) for t in trans_toks]\n",
    "trans_type_lens = [len(set(t)) for t in trans_toks]\n",
    "TTR = [len(set(t))/len(t) for t in trans_toks]\n",
    "\n",
    "conv_speeches['Toks'] = trans_toks\n",
    "conv_speeches['Token_count'] = trans_tok_lens\n",
    "conv_speeches['Type_count'] = trans_type_lens\n",
    "conv_speeches['TTR'] = TTR\n",
    "conv_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Toks</th>\n",
       "      <th>Token_count</th>\n",
       "      <th>Type_count</th>\n",
       "      <th>TTR</th>\n",
       "      <th>SentAmt</th>\n",
       "      <th>AVGSENTLEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "      <td>[good, evening, and, thank, you, to, everyone,...</td>\n",
       "      <td>193</td>\n",
       "      <td>118</td>\n",
       "      <td>0.611399</td>\n",
       "      <td>5</td>\n",
       "      <td>38.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "      <td>[we, climbed, the, impossible, mountain, and, ...</td>\n",
       "      <td>750</td>\n",
       "      <td>326</td>\n",
       "      <td>0.434667</td>\n",
       "      <td>55</td>\n",
       "      <td>13.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "      <td>[hello, america, im, andrew, yang, you, might,...</td>\n",
       "      <td>428</td>\n",
       "      <td>214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "      <td>17.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "      <td>[good, evening, everybody, as, youve, seen, by...</td>\n",
       "      <td>2278</td>\n",
       "      <td>795</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>102</td>\n",
       "      <td>22.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "      <td>[good, evening, our, great, nation, is, now, l...</td>\n",
       "      <td>982</td>\n",
       "      <td>440</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>44</td>\n",
       "      <td>22.318182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Aff                                             Speech  \\\n",
       "0   D  Good evening and thank you to everyone here to...   \n",
       "1   D  We climbed the impossible mountain, and right ...   \n",
       "2   D  Hello, America. I'm Andrew Yang. You might kno...   \n",
       "3   D  Good evening everybody. As you've seen by now ...   \n",
       "4   D  Good evening. Our great nation is now living i...   \n",
       "\n",
       "                                                Toks  Token_count  Type_count  \\\n",
       "0  [good, evening, and, thank, you, to, everyone,...          193         118   \n",
       "1  [we, climbed, the, impossible, mountain, and, ...          750         326   \n",
       "2  [hello, america, im, andrew, yang, you, might,...          428         214   \n",
       "3  [good, evening, everybody, as, youve, seen, by...         2278         795   \n",
       "4  [good, evening, our, great, nation, is, now, l...          982         440   \n",
       "\n",
       "        TTR  SentAmt  AVGSENTLEN  \n",
       "0  0.611399        5   38.600000  \n",
       "1  0.434667       55   13.636364  \n",
       "2  0.500000       25   17.120000  \n",
       "3  0.348990      102   22.333333  \n",
       "4  0.448065       44   22.318182  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [len(nltk.sent_tokenize(t)) for t in conv_speeches['Speech']]\n",
    "\n",
    "conv_speeches['SentAmt'] = sents\n",
    "conv_speeches['AVGSENTLEN'] = conv_speeches.Token_count/conv_speeches.SentAmt\n",
    "conv_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building logistic regression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "X = conv_speeches[['Token_count', 'Type_count', 'TTR', 'SentAmt', 'AVGSENTLEN']]\n",
    "y = conv_speeches['Aff']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(C = 1, class_weight= None, penalty='l2')   # default setting\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so for speeches, the svc classifier was almost 100% positive about the Affiliation, but the format of the speeches didn't play a big role. This might actually help my hypothesis in that speeches were more directed at known audience and so they were more polarizing, but the actually makings of the speeches were not as distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = ['Republican', 'Democrat', 'Republicans', 'Democrats']\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])\n",
    "conv_speeches[\"Speech2\"] = conv_speeches[\"Speech\"].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Toks</th>\n",
       "      <th>Token_count</th>\n",
       "      <th>Type_count</th>\n",
       "      <th>TTR</th>\n",
       "      <th>SentAmt</th>\n",
       "      <th>AVGSENTLEN</th>\n",
       "      <th>Speech2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "      <td>[good, evening, and, thank, you, to, everyone,...</td>\n",
       "      <td>193</td>\n",
       "      <td>118</td>\n",
       "      <td>0.611399</td>\n",
       "      <td>5</td>\n",
       "      <td>38.600000</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "      <td>[we, climbed, the, impossible, mountain, and, ...</td>\n",
       "      <td>750</td>\n",
       "      <td>326</td>\n",
       "      <td>0.434667</td>\n",
       "      <td>55</td>\n",
       "      <td>13.636364</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "      <td>[hello, america, im, andrew, yang, you, might,...</td>\n",
       "      <td>428</td>\n",
       "      <td>214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "      <td>17.120000</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "      <td>[good, evening, everybody, as, youve, seen, by...</td>\n",
       "      <td>2278</td>\n",
       "      <td>795</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>102</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "      <td>[good, evening, our, great, nation, is, now, l...</td>\n",
       "      <td>982</td>\n",
       "      <td>440</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>44</td>\n",
       "      <td>22.318182</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Aff                                             Speech  \\\n",
       "0   D  Good evening and thank you to everyone here to...   \n",
       "1   D  We climbed the impossible mountain, and right ...   \n",
       "2   D  Hello, America. I'm Andrew Yang. You might kno...   \n",
       "3   D  Good evening everybody. As you've seen by now ...   \n",
       "4   D  Good evening. Our great nation is now living i...   \n",
       "\n",
       "                                                Toks  Token_count  Type_count  \\\n",
       "0  [good, evening, and, thank, you, to, everyone,...          193         118   \n",
       "1  [we, climbed, the, impossible, mountain, and, ...          750         326   \n",
       "2  [hello, america, im, andrew, yang, you, might,...          428         214   \n",
       "3  [good, evening, everybody, as, youve, seen, by...         2278         795   \n",
       "4  [good, evening, our, great, nation, is, now, l...          982         440   \n",
       "\n",
       "        TTR  SentAmt  AVGSENTLEN  \\\n",
       "0  0.611399        5   38.600000   \n",
       "1  0.434667       55   13.636364   \n",
       "2  0.500000       25   17.120000   \n",
       "3  0.348990      102   22.333333   \n",
       "4  0.448065       44   22.318182   \n",
       "\n",
       "                                             Speech2  \n",
       "0  Good evening and thank you to everyone here to...  \n",
       "1  We climbed the impossible mountain, and right ...  \n",
       "2  Hello, America. I'm Andrew Yang. You might kno...  \n",
       "3  Good evening everybody. As you've seen by now ...  \n",
       "4  Good evening. Our great nation is now living i...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('svc', SVC())]),\n",
       "             n_jobs=3,\n",
       "             param_grid={'svc__C': [100000.0], 'svc__gamma': [1],\n",
       "                         'svc__kernel': ['linear'],\n",
       "                         'tfidf__max_features': [1000, 2000, 3000],\n",
       "                         'tfidf__stop_words': ['english', None]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "x = conv_speeches.Speech2[:1000] \n",
    "y = conv_speeches.Aff[:1000]\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "svc_model = SVC()\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps=[('tfidf', tfidf_model), ('svc', svc_model)])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 2000, 3000],        \n",
    "    'tfidf__stop_words': ['english', None],\n",
    "    'svc__C': [1E5],\n",
    "    'svc__kernel' : ['linear'],\n",
    "    'svc__gamma' : [1]\n",
    "}                           \n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=3, cv=5) \n",
    "search.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'svc__C': 100000.0, 'svc__gamma': 1, 'svc__kernel': 'linear', 'tfidf__max_features': 2000, 'tfidf__stop_words': None}\n",
      "best mean accuracy: 0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "print('best parameters:', search.best_params_)     # best-performing parameter combo\n",
    "print('best mean accuracy:', search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first: trying [tobigue's](https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers) code for an sklearn binary case informative features search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1bf9019ec4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m X_train, X_test, y_train, y_test = train_test_split(conv_speeches['Speech'], conv_speeches['Aff'], random_state=0,\n\u001b[1;32m      3\u001b[0m                                   train_size=0.6)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnbmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#using this to find vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_text_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv_speeches['Speech'], conv_speeches['Aff'], random_state=0,\n",
    "                                  train_size=0.6)\n",
    "nbmodel = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2,max_features=1500, stop_words='english') #using this to find vocab\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n",
    "X_text_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nbmodel.fit(X_text_train, y_train)\n",
    "pred = nbmodel.predict(X_text_test)\n",
    "\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "\n",
    "show_most_informative_features(vectorizer, nbmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it looks a little iffy (especially that left side). Apparently, it's because the nbmodel coefficient concatenates and merges into one array for binary cases. Seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, I decided to just add in a neutral affiliation with just a \".\" as the Speech text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "neutral = pd.DataFrame({'Aff' : 'None', 'Speech': '.'}, index=[1])\n",
    "conv2 = conv_speeches[['Aff', 'Speech']]\n",
    "conv2 = pd.concat([conv2, neutral])\n",
    "conv2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv2['Speech'], conv2['Aff'], random_state=0,\n",
    "                                  train_size=0.6)\n",
    "nbmodel = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2,max_features=1500, stop_words='english') #using this to find vocab\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n",
    "X_text_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nbmodel.fit(X_text_train, y_train)\n",
    "pred = nbmodel.predict(X_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmodel.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are three coefficent lists in the array, which allows me to use the important feature function that uses highest coefficent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top(vectorizer, clf, class_labels, n):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top = np.argsort(clf.coef_[i])[0-n:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j].replace(' ', '-') for j in top)))\n",
    "    \n",
    "\n",
    "print_top(vectorizer, nbmodel, nbmodel.classes_, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv_speeches['Speech'], conv_speeches['Aff'], random_state=0,\n",
    "                                  train_size=0.6)\n",
    "nbmodel = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2,max_features=1500, stop_words='english') #using this to find vocab\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n",
    "X_text_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nbmodel.fit(X_text_train, y_train)\n",
    "pred = nbmodel.predict(X_text_test)\n",
    "\n",
    "def print_top2(vectorizer, clf, class_labels, n):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top = np.argsort(clf.coef_[:750])[0-n:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j].replace(' ', '-') for j in top)))\n",
    "    \n",
    "\n",
    "print_top(vectorizer, nbmodel, nbmodel.classes_[0], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmodel.coef_[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top2(vectorizer, clf, class_labels, n):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top = np.argsort(clf.coef_[i][:750])[0-n:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j].replace(' ', '-') for j in top)))\n",
    "    \n",
    "\n",
    "print_top(vectorizer, nbmodel, nbmodel.classes_[1], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
