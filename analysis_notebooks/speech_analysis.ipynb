{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RNC and DNC Speech Analysis\n",
    "* The RNC and DNC speeches are written with the intent to persuade an otherwise already persuaded audience.\n",
    "    * For example, viewers of the RNC are typically republican: \"More viewers tuned into Fox News to watch the final night of the Republican convention than any other TV network\" (https://www.wsj.com/articles/rnc-ratings-final-night-draws-23-8-million-viewers-as-trump-accepts-nomination-11598652831)\n",
    "* This analysis will include a classifier which will predict whether a test document is from a republican speaker or a democratic speaker. \n",
    "    * Goal: to see if the classifier will be able to recognize differences in the word choices of each group. Hypothesis: high accuracy. Also, see the top features.\n",
    "* Additionally, I will include a model to see if the length of the speech, the length of the sentences, the TTR, etc. has any significance on predicting between republican and democratic speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns        # seaborn graphical package\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    30\n",
       "D    21\n",
       "Name: Aff, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_speeches = pd.read_csv(\"/Users/emmatarcson/Documents/data_science/RhetoricalFactor-analysis/data_sample/convspeeches.csv\")\n",
    "conv_speeches = conv_speeches[['Aff','Speech']]\n",
    "conv_speeches.Aff.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv_speeches['Speech'], conv_speeches['Aff'], random_state=0,\n",
    "                                  train_size=0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, max_features=4000, stop_words='english')\n",
    "\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100000.0, kernel='linear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svcmodel = SVC(kernel='linear', C=1E5)  \n",
    "svcmodel.fit(X_text_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29    R\n",
      "11    D\n",
      "10    D\n",
      "22    R\n",
      "2     D\n",
      "28    R\n",
      "45    R\n",
      "32    R\n",
      "26    R\n",
      "4     D\n",
      "33    R\n",
      "41    R\n",
      "27    R\n",
      "35    R\n",
      "34    R\n",
      "7     D\n",
      "14    D\n",
      "46    R\n",
      "18    D\n",
      "48    R\n",
      "Name: Aff, dtype: object\n",
      "['R' 'D' 'D' 'R' 'D' 'R' 'D' 'R' 'R' 'D' 'R' 'R' 'R' 'R' 'R' 'D' 'D' 'R'\n",
      " 'D' 'R']\n"
     ]
    }
   ],
   "source": [
    "pred = svcmodel.predict(X_text_test)\n",
    "print(y_test[:20])\n",
    "print(pred[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letsee = [x for x in pred if x == 'D']\n",
    "len(letsee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmatarcson/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['D', 'R'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEECAYAAACr5bh1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXT0lEQVR4nO3dfVRUdf4H8PedwUEZFmlMVxMV8KG1c35omo9Ju2UuWvlzNXWAGrWt0/r8AJabEpop1lHIop8PuamBpld26VedIjvWHik7Ulv5bJpWSygJCGdxQJyn+/vDdVb8hTMX7szc7/R+de7JC3Pv/YT67vt075UURVFARCQwQ6gLICJqKwYZEQmPQUZEwmOQEZHwGGREJLyIUBdwoybbfaEugVRov/mtUJdArRHVsU2Hz5Ri/P7sJqW+TdfyB1tkRCQ83bXIiEj/9NYCYpARkWoRkhTqEpphkBGRagZ95RiDjIjUY9eSiIRnYNeSiETHFhkRCY9jZEQkPCO7lkQkOnYtiUh47FoSkfDYIiMi4elt+YXegpWIBBAh+b/54/Dhw7DZbACAkydPIj09HTabDY8//jhqamp8Hs8gIyLVDCo2X7Zs2YKsrCxcuXIFALB69Wo8++yzKCwsxJgxY7Blyxa/6iEiUsUAye/Nl549eyI/P9+7n5eXh/79+wMA3G43IiMjfZ6DY2REpJqaWUtZliHLsnffarXCarV691NSUlBRUeHd79KlCwDgq6++wo4dO7Bz506f12CQEZFqarpyNwaXP95//31s3LgRr732GiwWi8/PM8iISLVAriN7++23IcsyCgsLERsb69cxDDIiUi1QD1Z0u91YvXo1unXrhnnz5gEAhgwZgvnz59+8noBUQ0RhTetZwri4OOzZswcA8Pnnn6s+nkFGRKrxFiUiEp4/yyqCiUFGRKqxRUZEwjMyyIhIdOxaEpHw2LUkIuHp7SZtBhkRqaazBhmDjIjU09uDFRlkRKQau5ZEJDx9tccYZETUChK7lkQkOn3FGIOMiFqBY2REJDyd9SwZZESkHm9RIiLh6SvGGGRE1Aq815KIhCfprE3GICMi1fQVYwwyImoFdi2JSHictSQi4ekrxhhkRNQKXBBLRMLTWY4xyIhIPb0tv9DbvZ9EJACj5P/mj8OHD8NmswEA/vnPfyItLQ3p6elYvnw5PB6Pz+MZZESkmqRi82XLli3IysrClStXAABr1qzBwoUL8eabb0JRFHz00Uc+z8EgIyLVJBX/+NKzZ0/k5+d7948fP46hQ4cCAO655x589tlnPs/BMbIAMyanwJiccnWnnQlSzz64Mu9hoLEhtIVRizweD1bkvIhTp7+FyWTCquxl6NWzR6jL0hU1s5ayLEOWZe++1WqF1Wr17qekpKCiosK7ryiK9wm0ZrMZly5d8nkNBlmAuT/ZC/cnewEAEdPnQyktYYjp3L6/74fD4YBcsBWHjhzFC3kvY+P6daEuS1fUdOVuDC6f5zb85+wNDQ2IiYnxeUxAguybb77B3r17UVdXh65du2Ls2LGIj48PxKWEISX0g6F7PBxvvBLqUsiHL78+hOSRIwAAA5P+C8dOnAxxRfoTyDnLO+64A2VlZRg2bBhKS0sxfPhwn8doPkZWUlKCpUuXolu3bkhOTobZbMa8efOwb98+rS8llIjxj8D1VkGoyyA/2BsaEB0d7d03Gg1wuVwhrEh/DJLk96bWkiVLkJ+fD6vVCqfTiZSUFJ/HaN4iKygowI4dOxAVFeX92sSJEzFr1izcf//9Wl9ODFFmSLf1gOfkoVBXQn6INpvRcF333+NREBHBUZjrad0ii4uLw549ewAACQkJ2LFjh6rjNW+RRURENAsxAIiOjobRaNT6UsIw3D4AnuNfhboM8tOggQNQ+unVmbJDR46iX5/eIa5IfyRJ8nsLBs3/N9NS4f4sagtXUrceUKoqQ10G+WnMfb/DgYNlSJ3+OBRFQc5z2aEuSXfC/jE+Z86cQWZmZrOvKYqCs2fPan0pYbjfl31/iHTDYDBgZdYzoS5D1ySdJZnmQbZ+/fqf/XpqaqrWlyKiEDHobCm95kF2bUUuEYWvYI19+YtTMUSkms5yjEFGROqxRUZEwtNZjjHIiEi91qzYDyQGGRGpZgj35RdEFP6kcF9+QUThj4P9RCQ8neUYg4yI1GOLjIiEp7McY5ARkXpGzloSkejYtSQi4eksxxhkRKQeg4yIhBf2D1YkovDHwX4iEh67lkQkPM5aEpHwdJZjDDIiUo8tMiISnpY55nQ68ec//xnnzp2DwWDA888/j9691b0UWWdPFSIiERiMkt+bL/v374fL5cLu3bsxZ86cFl8peTNskRGRalp2LRMSEuB2u+HxeGC32xERoT6WGGREpJ6KdWSyLEOWZe++1WqF1Wr17kdFReHcuXMYN24c6urqsGnTJtXlMMiISD0VLbIbg+tG27dvx6hRo5CZmYnKykpMnz4d7777LiIjI/2+BoOMiFTTsmsZExODdu3aAQA6duwIl8sFt9ut6hwMMiJSz6jdPOGMGTOwdOlSpKenw+l0YtGiRYiKilJ1DgYZEamm5U3jZrMZL7/8cpvOwSAjIvW4IJaIRCfMY3xyc3NbHNDLyMgIWEFEJABRWmSJiYnBrIOIRCJKi2zixIkAAJfLhbfeeguVlZUYNmwY+vbtG7TiiEifJA1nLbXgs5rly5fj/PnzOHDgABoaGrBkyZJg1EVEeiZJ/m9B4DPIysvLsWDBAphMJtx33324dOlSMOoiIh2TDP5vweBz1tLtdqO2thaSJMFut8Ng0FeTkohCQJTB/msWLlyItLQ0VFdXw2q1YunSpcGoi4h0TJjlF9cMHToUJSUlqKurg8Vi0d2TIYkoBHSWAz77ifv378eYMWPwxBNPYOzYsSgrKwtGXUSkY5LR4PcWDD5bZK+++iqKiopgsVhQXV2NOXPmYM+ePcGojYj0SrSupdlshsViAQB07twZHTp0CHhRRKRzOutathhkeXl5AK7OWv7pT3/C4MGDceTIEZhMpqAVR0T6pLex8haDLCEhodm/AWD06NGBr4iI9E+UruX1tygdPXoULpcLiqKgqqoqaMURkT7p7RYln2Nkc+fOhdPpRFVVFdxuN7p06YKHHnooGLURkV7prGvpM1btdjtef/11JCUlobi4GFeuXAlGXUSkY5JB8nsLBp8tsmvvmLt8+TLat28Pp9MZ8KKISOd01iLzGWRjxozBq6++it/85jeYOnUqzGZzMOoiIj3T2WC/pCiK4u+HT506hfj4eFXvm1NLufB9wM5N2pvVdUCoS6BW2KTUt+l41+wH/f5sxIb32nQtv67R0jcyMjJaXCuSm5sbsIKISACizFqmpqYGsw4iEokoY2RDhw4NZh1EJBJRgoyIqEU6e8Aqg4yI1BOtRXb69GmsWLECly5dwvjx49G3b1/ce++9waiNiPRK4yDbvHkzPv74YzidTqSlpWHKlCmqjvfZPly9ejXWrFmD2NhYTJ48Gfn5+a0ulojChNHo/+ZDWVkZvv76a+zatQuFhYX46aefVJfjV9eyV69ekCQJFouFC2KJSFWLTJZlyLLs3bdarbBard79Tz/9FP369cOcOXNgt9vx9NNPqy7HZ5B17NgRu3fvxuXLl/Hee+8hJiZG9UWIKMyoCLIbg+tGdXV1OH/+PDZt2oSKigrMmjULH3zwgapnnvnsWubk5KCiogK33HILjh07htWrV/t9ciIKUxq+oDc2NhajRo2CyWRCYmIiIiMjUVtbq6ocny2y+vp6pKene/cbGxsRGxur6iJEFGY0XH4xePBgFBQU4LHHHkNVVRUuX76sOmN8BtmiRYsgSRI8Hg8qKirQq1cv7Nq1q7U1E1E40DDI7r33XnzxxReYPHkyFEVBdnY2jH5MElzPZ5BdP0hXX1+P7Oxs9ZUSUXjRePlFawb4r6dqQeyvfvUrlJeXt+mCRCQ+SbSV/Var1Tt7cPHiRYwcOTLgRRGRzom2sj8nJwft27cHAERGRuLWW28NeFFEpHM6CzKf7cOsrCx0794d3bt3Z4gR0VUaLr/Qgs8WWVRUFHJycpCQkADDv/vFN1vcRkS/ACpnFQPNZ5DdeeedAK6OjxERAdBd19JnkBkMBsyePdu7z8dcE5EwQVZUVIS//vWvOHv2LEpLSwEAHo8HTqcTmZmZQSuQiHRIlOUXEyZMwIgRI7B582bMnDkTwNXWWadOnYJWHBHplCgtMpPJhLi4ODz//PPBrIeIRCBKkBERtUi0WUsiov+HLTIiEh6DjIiEJ8qsJRFRi9giIyLhGTjYT0SiM7BFRkSikzhGRkSi4xgZEQmPs5ZEJDy2yIhIeJy1JCLhsWtJRMJj15KIhKez5Rf6qoaIxGCQ/N/8cPHiRfz2t7/F2bNnW1UOW2REpJ6Gg/1OpxPZ2dne9+e2qhzNqiGiXw7J4P/mw4svvojU1FR06dKl1eUwyIhIPRVdS1mWMWnSJO8my7L3NMXFxbBYLEhOTm5TOZKiKEpb/5u0pFz4PtQlkAqzug4IdQnUCpuU+jYd7y7K8/uzxikZLX7vkUcegSRJkCQJJ0+eRHx8PDZu3IjOnTurqodjZESknkazljt37vT+2mazYcWKFapDDGCQEVFr8DE+RCS8ANyiVFhY2OpjGWREpJ7OFsQyyIhIPXYtiUh4bJERkfB0dtO4vmI1jB0+8Q1s858KdRnkQ/zQu5Dx9/cAAN36347Fn+zFU59+iLT/yYOks0fXhJTB4P8WjHKCcpVfuL+8WYSsF9fD4XCGuhS6id8/tQC2v+Qj4t/3/E3IWY7/Xfoc1o76PUxRHTDgvx8IcYU6YjD6vwWjHK1P6HK58OGHH+LgwYPer9XU1GDhwoVaX0oYPW7rhvxVz4a6DPKh+uz32DzpUe/+5ocfxZlPPoOxXTvEdP016i9UhbA6nZEk/7cg0HyMbPHixTAajaiursaZM2cQFxeHZcuWYdq0aVpfShgpvxuFisqfQl0G+fB18Tvo1Kund1/xeGDp2QML972Dy//6Fy6cOhPC6nRGZ91szYOsvLwcxcXFcDgcePjhh9GuXTsUFBSgd+/eWl+KKOBqy39Edr87cffj0zA5LwdvzJgZ6pL0IdwH+6OjowEAJpMJHo8HW7duZYiRkGa9vRtd+lz9s9t0yQ7F4wlxRTqi4WN8tBDQ5RedOnVCbGxsIC9BFDB7X8jD9O0b4XI44Gi8jMIn5oa6JP3Q2VuUNH+Mz8iRIzFixAgoioKDBw9ixIgR3u/l5ub6PJ6P8RELH+MjpjY/xqdU9v2hfzPeY23TtfyheYts/fr13l+npqZqfXoi0oNwX9k/dOhQrU9JRHqjs8F+3qJEROqFe4uMiMKfxBYZEQnPoK/o0Fc1RCQGPo+MiITHMTIiEh7HyIhIeGyREZHw2CIjIuEZ9XWvJYOMiNRj15KIhMeuJREJjy0yIhKehi0yp9OJpUuX4ty5c3A4HJg1axZGjx6t6hwMMiJSz6hddLzzzjuIjY3F2rVrUVdXh4kTJzLIiCjw1Nw0LssyZPk/D2K0Wq2wWv/zsMWxY8ciJSXFu29sxYwog4yI1FMxRnZjcN3IbDYDAOx2O+bPn9+qV0fqa8SOiMSg8XstKysrMW3aNEyYMAHjx49XXQ5bZESknoazljU1NfjjH/+I7OzsZu/4UIMtMiJST8MW2aZNm1BfX48NGzbAZrPBZrOhqalJXTlav0WprfgWJbHwLUpiautblJQfDvv9WSk+8H9G2LUkIvW4IJaIhMdblIhIfAwyIhIdW2REJDwGGREJj4P9RCQ8fTXIGGRE1Br6SjIGGRGpxzEyIhIeg4yIhMfBfiISH1tkRCQ6di2JSHgMMiISH4OMiASn5uUjwcAgIyL1OGtJRMJji4yIhMcgIyLxMciISHRskRGR8PSVYwwyImoFzloSkfDYtSQi8THIiEh0GrbIPB4PVqxYgVOnTsFkMmHVqlXo1auXqnPoq6NLRGKQJP83H/bt2weHwwFZlpGZmYkXXnhBdTlskRGRehoO9n/55ZdITk4GAAwcOBDHjh1TfQ7dBZn064RQl0AqbFLqQ10ChUJUR78/KssyZFn27lutVlitVu++3W5HdHS0d99oNMLlciEiwv940l2QEVF4uTG4bhQdHY2GhgbvvsfjURViAMfIiCjEBg0ahNLSUgDAoUOH0K9fP9XnkBRFUbQujIjIX9dmLU+fPg1FUZCTk4PevXurOgeDjIiEx64lEQmPQUZEwmOQEZHwuPwiwMrKyrBw4UL06dMHiqLA5XJh2rRpeOCBB0JdGv2M63+/AKChoQFxcXFYt24dTCZTiKujljDIgmD48OF46aWXAFz9i2Gz2ZCQkID+/fuHuDL6Odf/fgFAZmYmPv74Y4wdOzaEVdHNsGsZZGazGVarFR988EGoSyE/OBwOVFVVoWNH/1eyU/CxRRYCnTp1wvHjx0NdBrXg4MGDsNlsuHjxIgwGA6ZOnYoRI0aEuiy6CbbIQuD8+fPo2rVrqMugFgwfPhyFhYXYuXMn2rVrh7i4uFCXRD4wyILMbrejqKiI4y0CuOWWW7B27VpkZWWhqqoq1OXQTbBrGQTXuioGgwFutxvz5s1DYmJiqMsiP/Tp0wc2mw2rVq3CK6+8EupyqAW8RYmIhMeuJREJj0FGRMJjkBGR8BhkRCQ8BhkRCY9BRgCARYsWoaysDKWlpc1eFHEjWZbhdDr9OueuXbuQn5/f7GvFxcVYt25di8fk5+dj165dfp1fzWcpvHEdGTVzzz333PT7mzdvxh/+8IfgFEPkJwaZ4IqLi/HRRx/Bbrejrq4Oc+bMQUpKCh566CHEx8fDZDLhueeew7Jly1BXVwcAyMrKwu23346dO3eiqKgInTt3xsWLF73n++6777B48WJs2LAB+/btg9vtRlpaGoxGI6qrq7Fo0SJs2LABubm5+OKLL6AoCmbMmIFx48bhH//4B3JyctCxY0cYDAYMHDiwxdpzc3Nx7NgxNDQ0oHfv3lizZg2Aqy9sLSkpQVNTE7KyspCUlISSkhJs374dBoMBgwcPxuLFiwP+syVxMMjCQGNjI7Zt24ba2lpMmTIFo0ePRmNjI2bPno077rgDa9euxfDhw5Geno4ffvgBzzzzDF577TUUFBTg3XffhSRJmDRpUrNznjhxAqWlpSgqKoLD4UBubi6WLVuGjRs34qWXXsL+/ftRUVGB3bt348qVK5g6dSruvvturFmzBrm5uUhISMDy5ctbrNlutyMmJgbbtm2Dx+PBgw8+iAsXLgAAunfvjpUrV+Lbb7/F008/jW3btiE/Px9/+9vf0KFDBzz11FM4cOBAQH+mJBYGWRgYMmQIDAYDbr31VsTExKC2thYAkJBw9WXHp0+fxsGDB1FSUgIAqK+vx3fffYc+ffp4HxaYlJTU7Jzff/89kpKSYDQa0aFDB2RlZTX7/unTp3H8+HHYbDYAgMvlwvnz53HhwgXvdQcNGoTy8vKfrTkyMhK1tbXIyMhAVFQUGhsbvWNvQ4YMAQD07dsX1dXVKC8vR21tLZ588kkAV5/p9uOPP7bth0ZhhYP9YeDaI4Fqampgt9vRqVMnAIDBcPW3NzExETNmzEBhYSHWr1+P8ePHo0ePHjhz5gyamprgdrtx8uTJZudMTEzEiRMn4PF44HQ68dhjj8HhcECSJHg8HiQmJmLYsGEoLCzEG2+8gXHjxiEuLg6dO3fG2bNnAQBHjx5tsebS0lJUVlYiLy8PGRkZaGpqwrW75Y4cOQIAOHXqFG677TbExcWhW7du2Lp1KwoLC/Hoo49iwIAB2v4QSWhskYWBmpoaTJ8+HZcuXcLy5cthNBqbfX/mzJlYtmwZ9uzZA7vdjrlz58JisWDBggVITU2FxWJBhw4dmh3Tv39/JCcnIy0tDR6PB2lpaTCZTLjrrrvw5JNPoqCgAJ9//jnS09PR2NiI+++/H9HR0Vi7di2WLFkCs9kMs9nc4gMJk5KSsGHDBkydOhUmkwk9evTwPmGioqIC06ZNg8PhwMqVK2GxWDBjxgzYbDa43W50794d48aNC8wPk4TEm8YFd/3gPNEvFbuWRCQ8tsiISHhskRGR8BhkRCQ8BhkRCY9BRkTCY5ARkfD+D9LYC/XfOyuqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels = ['D','R']\n",
    "mat = confusion_matrix(y_test, pred, labels)\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cmap=\"Reds\",\n",
    "           xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So there is one instance of it guessing Democratic, instead of Republican in the test set. \n",
    "* this makes me wonder if there is some super significant word in these speeches that is causing this, but vvv\n",
    "* That being said, I did expect a higher accuracy from the speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because this is also just two binary, I can't do feature extraction, which is a bummer, but i'll add a random neutral \n",
    "# file just to see, not exactly pro data science, but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>R</td>\n",
       "      <td>Good evening. My name is Rudy Giuliani, and I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>R</td>\n",
       "      <td>I'm Tiffany Trump. Since speaking at the Repub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>R</td>\n",
       "      <td>Good evening. I'm Senator Tim Scott from the g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>R</td>\n",
       "      <td>Good evening. I'm Senator Tom Cotton. A lot's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Yo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Aff                                             Speech\n",
       "47     R  Good evening. My name is Rudy Giuliani, and I'...\n",
       "48     R  I'm Tiffany Trump. Since speaking at the Repub...\n",
       "49     R  Good evening. I'm Senator Tim Scott from the g...\n",
       "50     R  Good evening. I'm Senator Tom Cotton. A lot's ...\n",
       "1   None                                                 Yo"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "neutral = pd.DataFrame({'Aff' : 'None', 'Speech': 'Yo'}, index=[1])\n",
    "conv2 = conv_speeches[['Aff', 'Speech']]\n",
    "conv2 = pd.concat([conv2, neutral])\n",
    "conv2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: immigrants, doesn, protect, times, high, including, trust, justice, dream, economic, failed, injustice, empathy, beau, left, hope, small, plan, look, children, says, lead, loss, black, stand, government, heard, pandemic, movement, women, day, strong, rights, truth, win, poverty, promise, voting, say, knows, crisis, office, mail, text, states, seen, didn, harris, kids, virus, million, healthcare, covid, jobs, united, got, going, democracy, ll, kamala\n",
      "None: forget, gone, goes, god, goal, globe, global, giving, gives, given, girl, going, fact, foreign, force, feels, feel, federal, fear, favorite, favored, father, fast, farmers, far, families, fall, fake, faith, fairness, fair, failure, failed, facts, fellow, felt, fields, fierce, food, follow, folks, focus, floyd, florida, flew, flag, fixed, forced, firsthand, finish, fine, finds, finding, financial, finally, final, filled, fighting, fired, zones\n",
      "R: pandemic, coming, means, reform, single, election, bless, isis, important, drug, law, care, middle, land, hunter, north, took, chinese, school, states, promise, believes, father, women, trust, year, ll, proud, matter, mother, called, ask, party, government, look, united, say, media, children, families, democrat, help, cities, war, home, choice, think, democrats, police, going, city, black, god, jobs, thank, dream, opportunity, heroes, freedom, china\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(conv2['Speech'], conv2['Aff'], random_state=0,\n",
    "                                  train_size=0.6)\n",
    "nbmodel = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2,max_features=1500, stop_words='english') #using this to find vocab\n",
    "X_text_train = vectorizer.fit_transform(X_train)\n",
    "X_text_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "nbmodel.fit(X_text_train, y_train)\n",
    "pred = nbmodel.predict(X_text_test)\n",
    "\n",
    "def print_top(vectorizer, clf, class_labels, n):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top = np.argsort(clf.coef_[i])[0-n:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \", \".join(feature_names[j].replace(' ', '-') for j in top)))\n",
    "    \n",
    "\n",
    "print_top(vectorizer, nbmodel, nbmodel.classes_, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Toks</th>\n",
       "      <th>Token_count</th>\n",
       "      <th>Type_count</th>\n",
       "      <th>TTR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "      <td>[good, evening, and, thank, you, to, everyone,...</td>\n",
       "      <td>193</td>\n",
       "      <td>118</td>\n",
       "      <td>0.611399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "      <td>[we, climbed, the, impossible, mountain, and, ...</td>\n",
       "      <td>750</td>\n",
       "      <td>326</td>\n",
       "      <td>0.434667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "      <td>[hello, america, im, andrew, yang, you, might,...</td>\n",
       "      <td>428</td>\n",
       "      <td>214</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "      <td>[good, evening, everybody, as, youve, seen, by...</td>\n",
       "      <td>2278</td>\n",
       "      <td>795</td>\n",
       "      <td>0.348990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "      <td>[good, evening, our, great, nation, is, now, l...</td>\n",
       "      <td>982</td>\n",
       "      <td>440</td>\n",
       "      <td>0.448065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Aff                                             Speech  \\\n",
       "0   D  Good evening and thank you to everyone here to...   \n",
       "1   D  We climbed the impossible mountain, and right ...   \n",
       "2   D  Hello, America. I'm Andrew Yang. You might kno...   \n",
       "3   D  Good evening everybody. As you've seen by now ...   \n",
       "4   D  Good evening. Our great nation is now living i...   \n",
       "\n",
       "                                                Toks  Token_count  Type_count  \\\n",
       "0  [good, evening, and, thank, you, to, everyone,...          193         118   \n",
       "1  [we, climbed, the, impossible, mountain, and, ...          750         326   \n",
       "2  [hello, america, im, andrew, yang, you, might,...          428         214   \n",
       "3  [good, evening, everybody, as, youve, seen, by...         2278         795   \n",
       "4  [good, evening, our, great, nation, is, now, l...          982         440   \n",
       "\n",
       "        TTR  \n",
       "0  0.611399  \n",
       "1  0.434667  \n",
       "2  0.500000  \n",
       "3  0.348990  \n",
       "4  0.448065  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so onto extra features\n",
    "\n",
    "import re\n",
    "trans_toks = [nltk.word_tokenize(re.sub(r'[^\\w+ ]', '', t.lower())) for t in conv_speeches['Speech']]\n",
    "trans_tok_lens = [len(t) for t in trans_toks]\n",
    "trans_type_lens = [len(set(t)) for t in trans_toks]\n",
    "TTR = [len(set(t))/len(t) for t in trans_toks]\n",
    "\n",
    "conv_speeches['Toks'] = trans_toks\n",
    "conv_speeches['Token_count'] = trans_tok_lens\n",
    "conv_speeches['Type_count'] = trans_type_lens\n",
    "conv_speeches['TTR'] = TTR\n",
    "conv_speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be some information to see if a classifier would be able to predict based on token length, but since these are pre written speeches, and I can't do feature extraction at the moment, maybe TTR will be useful to look at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aff</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Toks</th>\n",
       "      <th>Token_count</th>\n",
       "      <th>Type_count</th>\n",
       "      <th>TTR</th>\n",
       "      <th>SentAmt</th>\n",
       "      <th>AVGSENTLEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening and thank you to everyone here to...</td>\n",
       "      <td>[good, evening, and, thank, you, to, everyone,...</td>\n",
       "      <td>193</td>\n",
       "      <td>118</td>\n",
       "      <td>0.611399</td>\n",
       "      <td>5</td>\n",
       "      <td>38.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>We climbed the impossible mountain, and right ...</td>\n",
       "      <td>[we, climbed, the, impossible, mountain, and, ...</td>\n",
       "      <td>750</td>\n",
       "      <td>326</td>\n",
       "      <td>0.434667</td>\n",
       "      <td>55</td>\n",
       "      <td>13.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>Hello, America. I'm Andrew Yang. You might kno...</td>\n",
       "      <td>[hello, america, im, andrew, yang, you, might,...</td>\n",
       "      <td>428</td>\n",
       "      <td>214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>25</td>\n",
       "      <td>17.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening everybody. As you've seen by now ...</td>\n",
       "      <td>[good, evening, everybody, as, youve, seen, by...</td>\n",
       "      <td>2278</td>\n",
       "      <td>795</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>102</td>\n",
       "      <td>22.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Good evening. Our great nation is now living i...</td>\n",
       "      <td>[good, evening, our, great, nation, is, now, l...</td>\n",
       "      <td>982</td>\n",
       "      <td>440</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>44</td>\n",
       "      <td>22.318182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Aff                                             Speech  \\\n",
       "0   D  Good evening and thank you to everyone here to...   \n",
       "1   D  We climbed the impossible mountain, and right ...   \n",
       "2   D  Hello, America. I'm Andrew Yang. You might kno...   \n",
       "3   D  Good evening everybody. As you've seen by now ...   \n",
       "4   D  Good evening. Our great nation is now living i...   \n",
       "\n",
       "                                                Toks  Token_count  Type_count  \\\n",
       "0  [good, evening, and, thank, you, to, everyone,...          193         118   \n",
       "1  [we, climbed, the, impossible, mountain, and, ...          750         326   \n",
       "2  [hello, america, im, andrew, yang, you, might,...          428         214   \n",
       "3  [good, evening, everybody, as, youve, seen, by...         2278         795   \n",
       "4  [good, evening, our, great, nation, is, now, l...          982         440   \n",
       "\n",
       "        TTR  SentAmt  AVGSENTLEN  \n",
       "0  0.611399        5   38.600000  \n",
       "1  0.434667       55   13.636364  \n",
       "2  0.500000       25   17.120000  \n",
       "3  0.348990      102   22.333333  \n",
       "4  0.448065       44   22.318182  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [len(nltk.sent_tokenize(t)) for t in conv_speeches['Speech']]\n",
    "\n",
    "conv_speeches['SentAmt'] = sents\n",
    "conv_speeches['AVGSENTLEN'] = conv_speeches.Token_count/conv_speeches.SentAmt\n",
    "conv_speeches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building logistic regression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "X = conv_speeches[['Token_count', 'Type_count', 'TTR', 'SentAmt', 'AVGSENTLEN']]\n",
    "y = conv_speeches['Aff']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(C = 1, class_weight= None, penalty='l2')   # default setting\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred = lr_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so for speeches, the svc classifier was almost 100% positive about the Affiliation, but the format of the speeches didn't play a big role. This might actually help my hypothesis in that speeches were more directed at known audience and so they were more polarizing, but the actually makings of the speeches were not as distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration, please ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible topic modeling for looking at what both groups were talking about? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a mess\n",
    "just_D = conv_speeches[conv_speeches.Aff == 'D'].Speech\n",
    "just_R = conv_speeches[conv_speeches.Aff == 'R'].Speech\n",
    "D_full = pd.DataFrame({'Aff': 'D', 'Speech' : str(list(just_D.values)).replace(\"',\", '.').replace(\" '\",\" \").strip('[]').strip(\"''\")},\n",
    "                      index = [1])\n",
    "R_full = pd.DataFrame({'Aff': 'R', 'Speech' : str(list(just_R.values)).replace(\"',\", '.').replace(\" '\",\" \").strip('[]').strip(\"''\")},\n",
    "                      index = [2])\n",
    "D_and_R = pd.concat([R_full, D_full])\n",
    "D_list = [('D', str(list(just_D.values)).replace(\"',\", '.').replace(\" '\",\" \").strip('[]').strip(\"''\"))]\n",
    "R_list = ('R', str(list(just_R.values)).replace(\"',\", '.').replace(\" '\",\" \").strip('[]').strip(\"''\"))\n",
    "D_list.append(R_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like in the debate analysis, there's no way to figure out a binary word importance :(\n",
    "# maybe I can find just the most frequent words used in each and compare them?\n",
    "import re\n",
    "\n",
    "# i tried getting rid of stop words using nltk, but that didn't work so I manually got rid of some\n",
    "#words = nltk.word_tokenize(re.sub(r'[^\\w ]', '', \n",
    "#word_features = nltk.FreqDist(t.lower() for t in nltk.word_tokenize(full))\n",
    "#def document_features(document): \n",
    "   # document_words = set(document) \n",
    "   # features = {}\n",
    "   # for word in word_features:\n",
    "    #    features['contains({})'.format(word)] = (word in document_words)\n",
    "   # return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featuresets = [(a, document_features(b)) for (a,b) in D_list]\n",
    "#train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "#classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
